behaviors:
  TowerAgent:
    trainer_type: ppo  # Proximal Policy Optimization
    hyperparameters:
      batch_size: 64  # 学習のために一度に処理するデータ数
      buffer_size: 2048  # 1エポックで使用されるデータ量
      learning_rate: 3.0e-4  # 学習率
      beta: 5.0e-4  # エージェントがより探索するか、より安定化するかを制御
      epsilon: 0.2  # 近接ポリシー最適化で使用する変数
      lambd: 0.95  # エージェントの報酬をディスカウントする率
      num_epoch: 3  # 各エポックの繰り返し数
      learning_rate_schedule: linear  # 学習率の減少の仕方
    network_settings:
      normalize: false  # 観測データを正規化するかどうか
      hidden_units: 128  # ニューラルネットワークの隠れ層のサイズ
      num_layers: 2  # 隠れ層の数
    reward_signals:
      extrinsic:
        gamma: 0.99  # 割引率
        strength: 1.0  # 外部報酬の強度
    max_steps: 500000  # トレーニング全体での最大ステップ数
    time_horizon: 64  # バッファが破棄されるまでに収集するステップ数
    summary_freq: 10000  # 何ステップごとに結果を記録するか
